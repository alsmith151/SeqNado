import os
import sys
import shutil
from datetime import datetime
import glob
from snakemake.utils import min_version
import ngs_pipeline.utils as utils
import pandas as pd

start_time = datetime.now()
ASSAY = "RNA"


configfile: "config_rna.yml"


container: "library://asmith151/ngs-pipeline/ngs:latest"


utils.format_config_dict(config)
pipeline_tools = utils.get_pipeline_tools(config)

# Methods to use
PILEUP_METHODS = [
    method for method in ["deeptools", "homer"] if pipeline_tools[method] == True
]
DUPLICATE_REMOVAL_METHODS = {
    "picard": "../rules/remove_pcr_duplicates_picard.smk",
    "deeptools": "../rules/remove_pcr_duplicates_deeptools.smk",
}
DUPLICATE_REMOVAL_RULE = DUPLICATE_REMOVAL_METHODS.get(
    config["remove_pcr_duplicates_method"], "../rules/remove_pcr_duplicates_ignore.smk"
)


# Get dataframe of samples
if os.path.exists(config["design"]):
    # Expect columns - sample fq1 fq2
    RNA_SAMPLES = utils.GenericFastqSamples(
        pd.read_csv(config["design"], sep="[\s+,\t]", engine="python")
    )

else:
    # Use pattern matching to get samples
    fq_files = list(utils.get_fastq_files("."))
    if fq_files:
        RNA_SAMPLES = utils.GenericFastqSamples.from_files(fq_files)
    else:
        raise ValueError("No FASTQ files found in the working directory")


RNA_SAMPLES.symlink_fastq_files(outdir="fastq")


DESIGN = RNA_SAMPLES.design
FASTQ_FILES = RNA_SAMPLES.fastq_files
SAMPLE_NAMES = RNA_SAMPLES.sample_names_all


include: "../rules/qc.smk"
include: "../rules/read_trimming.smk"
include: "rules/align.smk"
include: DUPLICATE_REMOVAL_RULE
include: "../rules/alignment_post_processing.smk"
include: "rules/alignment_counts.smk"
include: "rules/pileup.smk"
include: "../rules/hub.smk"


rule all:
    input:
        multiqc_fastq_raw="qc/fastq_qc_raw_report.html",
        multiqc_fastq_trimmed="qc/fastq_qc_trimmed_report.html",
        multiqc_bam_raw="qc/bam_raw_qc_report.html",
        multiqc_bam_filtered="qc/bam_filtered_qc_report.html",
        mutiqc_full="qc/full_qc_report.html",

        read_counts = "feature_counts/read_counts.tsv",


        pileups=expand(
            "bigwigs/{method}/{sample}_{strand}.bigWig",
            sample=SAMPLE_NAMES,
            method=PILEUP_METHODS,
            strand=["plus", "minus"]
        ),

        hub=os.path.join(
            config["ucsc_hub_details"]["directory"],
            f"{config['ucsc_hub_details']['name']}.hub.txt",
        ),


onsuccess:
    slurm_files = glob.glob("slurm-*.out")
    sps_files = glob.glob("sps-*")

    for fn in [*slurm_files, *sps_files]:
        try:
            if not os.path.isdir(fn):
                os.remove(fn)
            else:
                shutil.rmtree(fn)

        except Exception as e:
            print(e)
